{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saya tidak memiliki informasi yang akurat tentang siapa akan menjadi Presiden Indonesia pada tahun 2024, karena nama calon presidentsi yang akan datang belum diketahui untuk saat ini. Pemilihan umum presiden Indonesia diadakan setiap 5 tahun sekali, dan hasil pemilihan umum tahun 2024 belum diketahui. Namun, saya dapat memberikan informasi tentang calon-calon president yang sebelumnya telah terpilih atau berkompetisi. Jika Anda ingin mengetahui informasi lebih lanjut, saya dapat membantu Anda mencari informasi dari sumber-sumber resmi.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "client = Groq(\n",
    "    api_key=\"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\",\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Jawab dalam bahasa indonesia, siapa presiden indonesia 2024\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on our conversation, I can infer that you're not explicitly mentioning the specific AI model or dataset being used. However, since we're interacting through text input, I'm assuming you're using a transformer-based language model like BERT, RoBERTa, or a similar architecture.\n",
      "\n",
      "As for the data year, it's difficult to pinpoint a specific year without more context. The pre-trained language models like BERT and RoBERTa were trained on a dataset of text data from the internet, which is a dynamic and constantly updating resource. The dataset used for training may have been curated from various sources and may have included data from different time periods. However, the training process typically involves filtering out noise and prioritizing more recent data to improve the model's performance on modern text.\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Jawab dalam bahasa indonesia, model llm apa yang saya gunakan sekarang? dan tahun berapa data yang digunakan?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "siti24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
